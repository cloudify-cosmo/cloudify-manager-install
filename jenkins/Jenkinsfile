def configuration = [vaultUrl: "${VAULT_URL}",  vaultCredentialId: "vault-app-role", engineVersion: 2]

def secrets = [
  [path: 'secret/jenkins/cloudify', engineVersion: 2, secretValues: [
    [envVar: 'OS_USERNAME', vaultKey: 'username'],
    [envVar: 'OS_TENANT', vaultKey: 'tenant'],
    [envVar: 'OS_IP', vaultKey: 'ip'],
    [envVar: 'OS_PASSWORD', vaultKey: 'password']]],
]

@Library('pipeline-shared-library') _

pipeline {
  agent {
    kubernetes {
      label "manager-install-mb-${env.BUILD_NUMBER}"
      defaultContainer 'jnlp'
      yamlFile 'jenkins/build-pod.yaml'
    }
  }

  options {
    checkoutToSubdirectory('cloudify-manager-install')
    buildDiscarder(logRotator(numToKeepStr:'10'))
    timeout(time: 60, unit: 'MINUTES')
    timestamps()
  }

  environment {
    PROJECT = "cloudify-manager-install"
    PATH = "/root/.local/bin:$PATH"
    VERSION = getVersion("master").trim()
    PRERELEASE = getPreRelease("master").trim()
    CLOUDIFY_VERSION = "${env.VERSION}"
    CLOUDIFY_PACKAGE_RELEASE = "${env.PRERELEASE}"
    S3_BUILD_PATH = "${env.VERSION}/${env.PRERELEASE}-build/${env.PROJECT}/${env.BRANCH_NAME}/${env.BUILD_NUMBER}"
  }
  stages{
    stage ('compatability and flake8') {
      parallel{
        stage ('py3_compat'){
          steps{
            sh script: "mkdir -p ${env.WORKSPACE}/py3_compat && cp -rf ${env.WORKSPACE}/${env.PROJECT}/. ${env.WORKSPACE}/py3_compat", label: "copying repo to seperate workspace"
            container('py27'){
              dir("${env.WORKSPACE}/py3_compat"){
                py3Compat()
              }
            }
          }
        }
        stage('flake8') {
          steps{
            sh script: "mkdir -p ${env.WORKSPACE}/flake8 && cp -rf ${env.WORKSPACE}/${env.PROJECT}/. ${env.WORKSPACE}/flake8", label: "copying repo to seperate workspace"
            container('py36'){
              dir("${env.WORKSPACE}/flake8") {
                echo 'install flake 8'
                sh 'pip install flake8 --user'
                echo 'run flake8'
                sh 'python -m flake8'
              }
            }
          }
        }
        stage('fetch_rpms') {
          steps {
            sh script:"mkdir -p ${env.WORKSPACE}/rpms", label: "creating RPMs folder"
            container('python') {
              dir("${env.WORKSPACE}/rpms") {
                echo 'Install requests'
                sh 'pip install requests'
                sh script:"""
                  for repo in cloudify-rest-service cloudify-premium cloudify-cli cloudify-management-worker; do
                    echo "fetching \$repo RPM"
                    curl https://cloudify-release-eu.s3.amazonaws.com/cloudify/${env.VERSION}/${env.PRERELEASE}-release/\$repo-${env.VERSION}-${env.PRERELEASE}.el7.x86_64.rpm -o \$repo-${env.VERSION}-${env.PRERELEASE}.el7.x86_64.rpm
                  done
                  for repo in cloudify-manager-ip-setter cloudify-rabbitmq; do
                    echo "fetching \$repo RPM"
                    curl https://cloudify-release-eu.s3.amazonaws.com/cloudify/${env.VERSION}/${env.PRERELEASE}-release/\$repo-${env.VERSION}-${env.PRERELEASE}.el7.noarch.rpm -o \$repo-${env.VERSION}-${env.PRERELEASE}.el7.noarch.rpm
                  done
                  curl https://cloudify-release-eu.s3.amazonaws.com/cloudify/${env.VERSION}/${env.PRERELEASE}-release/cloudify-agents-${env.VERSION}-${env.PRERELEASE}.el7.centos.noarch.rpm -o cloudify-agents-${env.VERSION}-${env.PRERELEASE}.el7.centos.noarch.rpm
                  curl https://cloudify-release-eu.s3.amazonaws.com/cloudify/${env.VERSION}/${env.PRERELEASE}-release/patroni-1.6.3-1.el7.x86_64.rpm -o patroni-1.6.3-1.el7.x86_64.rpm
                  """, label: 'Fetch RPM built on Jenkins located on S3'
              }
            }
          }
        }
        stage('build_prometheus_rpms') {
          steps {
            sh script: "mkdir -p ${env.WORKSPACE}/prometheus && cp -rf ${env.WORKSPACE}/${env.PROJECT}/. ${env.WORKSPACE}/prometheus", label: "copying repo to seperate workspace"
            container('rpmbuild') {
              dir("${env.WORKSPACE}/prometheus") {
                echo 'Link items from workspace under rpm sources dir'
                sh """
                  rm ~/rpm -fr
                  ln -s ${env.WORKSPACE}/rpms ./rpms
                  ln -s . ~/rpm
                  """
                echo 'Install rpmdevtools and build dependencies'
                sh '''
                  yum install rpmdevtools -y
                  yum-builddep -y packaging/install_rpm.spec
                  '''
                echo 'build rpms'
                sh """
                  for file in prometheus prometheus_node_exporter prometheus_postgres_exporter prometheus_blackbox_exporter; do
                    echo "build \$file rpm"
                    rpmbuild -bb packaging/\$file.spec
                  done
                  """
                sh script:("mkdir -p ${env.WORKSPACE}/prometheus-rpms && cp -rf /root/rpmbuild/RPMS/x86_64/*.x86_64.rpm ${env.WORKSPACE}/prometheus-rpms"), label:'Copy RPMS to rpms folder'
              }
            }
          }
          post {
            success {
              archiveArtifacts '**/prometheus-rpms/*.rpm'
            }
          }
        }
      }
    }
    stage('build_rpm & deploy ec2 instance') {
      parallel {
        stage('build_rpm') {
          steps {
            container('rpmbuild') {
              sh """
                cd && cd rpmbuild
                git clone --single-branch --branch ${env.BRANCH_NAME} https://github.com/cloudify-cosmo/cloudify-manager-install.git SOURCES && cd SOURCES
                """
              echo 'Setup Github SSH key'
              setupGithubSSHKey()
              echo 'Link items from workspace under rpm sources dir'
              sh """
                cd /root/rpmbuild/SOURCES
                rm ~/rpm -fr
                ln -s ${env.WORKSPACE}/rpms ./rpms
                cp -a ${env.WORKSPACE}/prometheus-rpms/prometheus-*.rpm ./rpms/
                cp -a ${env.WORKSPACE}/prometheus-rpms/*_exporter-*.rpm ./rpms/
                ln -s . ~/rpm
                """
              sh script: """
                pushd rpms
                  /root/rpmbuild/SOURCES/packaging/fetch_requirements --edition premium -b ${BRANCH_NAME} >~/fetch_requirements.log
                  cat ~/fetch_requirements.log
                popd
                pwd
                ls
                """, label: 'fetching requirements'
              echo 'prepare to build rpm'
              sh '''
                yum install rpmdevtools -y
                chmod a+wx /opt
                '''
              echo 'Installing build dependencies'
              sh 'yum-builddep -y /root/rpmbuild/SOURCES/packaging/install_rpm.spec'
              echo 'set premium_edition in the packaged config.yaml'
              sh 'sed -i "s/set_by_installer_builder/premium/" /root/rpmbuild/SOURCES/config.yaml'
              echo 'Build RPM'
              sh """
                cd /root/rpmbuild/SOURCES/
                rpmbuild -D "CLOUDIFY_VERSION ${CLOUDIFY_VERSION}" \
                -D "CLOUDIFY_PACKAGE_RELEASE ${CLOUDIFY_PACKAGE_RELEASE}" \
                -bb packaging/install_rpm.spec
                """
              sh script:("mkdir -p ${env.WORKSPACE}/manager-rpm && cp -rf /root/rpmbuild/RPMS/x86_64/*-manager-*.x86_64.rpm ${env.WORKSPACE}/manager-rpm"), label:'Copy RPMS to rpms folder'
            }
          }
          post {
            success {
              echo 'Upload artifacts to S3'
              uploadToReleaseS3("${env.WORKSPACE}/manager-rpm/","${env.S3_BUILD_PATH}")
            }
          }
        }
        stage('Deploy ec2 instance'){
          steps {
            script {
              buildState = 'FAILURE'
              catchError(message: 'Failure on: manager-install EC2 Creation', buildResult: 'SUCCESS', stageResult: 'FAILURE') {
                container('py27') {
                  echo 'Setup Github SSH key'
                  setupGithubSSHKey()
                  dir("${env.WORKSPACE}/${env.PROJECT}/jenkins") {
                    withVault([configuration: configuration, vaultSecrets: secrets]){
                      sh script:"""#!/bin/bash
                      apt-get update
                      python -m ensurepip --upgrade
                      python -m pip install --upgrade pip
                      python -m pip install --upgrade virtualenv

                      virtualenv .venv
                      source .venv/bin/activate

                      pip install cloudify==5.1.0
                      cfy profile use ${env.OS_IP} -u ${env.OS_USERNAME} -p ${env.OS_PASSWORD} -t ${env.OS_TENANT}
                      pushd 'bp'
                        cfy install -b ec2-manager-install-blueprint-${env.BRANCH_NAME}-${env.BUILD_NUMBER} ec2-manager-install-blueprint.yaml
                      popd
                      cfy deployments capabilities ec2-manager-install-blueprint-${env.BRANCH_NAME}-${env.BUILD_NUMBER} --json > capabilities.json
                      echo -e \$(cat capabilities.json | jq '.key_content.value' | tr -d '"') > ~/.ssh/ec2_ssh_key && chmod 600 ~/.ssh/ec2_ssh_key
                      sleep 160
                      ssh-keyscan -H \$(cat capabilities.json | jq '.endpoint.value' | tr -d '"') >> ~/.ssh/known_hosts
                      echo 'ClientAliveInterval 50' | sudo tee --append /etc/ssh/sshd_config
                      """, label:'Configure and install blueprint on manager'
                    }
                  }
                }
              }
            }
          }             
        }
      }
    }
        stage('install_manager') {
          environment {
            CONTAINER_NAME = "cfy_aio"
            IMAGE_NAME = "cfy_manager_image"
          }
          steps {
            catchError(message: 'Failure on: manager-install install_manager', buildResult: 'SUCCESS', stageResult: 'FAILURE') {
              container('py27') {
                dir("${env.WORKSPACE}/${env.PROJECT}/jenkins") {
                  withVault([configuration: configuration, vaultSecrets: secrets]){
                    sh script: """#!/bin/bash
                    ssh -i ~/.ssh/ec2_ssh_key -l centos \$(cat capabilities.json | jq '.endpoint.value' | tr -d '"') /bin/bash << 'EOT'
sudo yum -y update
sudo service docker start
git clone https://github.com/cloudify-cosmo/cloudify-manager-install.git && cd cloudify-manager-install && git checkout ${env.BRANCH_NAME}
docker run --net host -dt -v ${pwd}:/mount python python -m http.server -d /mount
echo 'Build manager container'
set -eux
ls -la .
pushd packaging/docker
  docker build --network host --build-arg rpm_file=https://cloudify-release-eu.s3.amazonaws.com/cloudify/${env.S3_BUILD_PATH}/cloudify-manager-install-${env.VERSION}-${env.PRERELEASE}.el7.centos.x86_64.rpm --tag ${env.IMAGE_NAME} .
popd
echo 'Run manager container'
set -eux
docker run --name ${env.CONTAINER_NAME} -d -v /sys/fs/cgroup:/sys/fs/cgroup:ro --tmpfs /run --tmpfs /run/lock ${env.IMAGE_NAME}
echo "Waiting for ${env.CONTAINER_NAME} to start"
docker exec ${env.CONTAINER_NAME} cfy_manager wait-for-starter
echo 'Check Manager status'
sleep 20
sudo chmod +x jenkins/validate_status.sh
./jenkins/validate_status.sh ${env.CONTAINER_NAME}
echo 'Run the Sanity check'
sudo docker exec ${env.CONTAINER_NAME} cfy_manager sanity-check
""", label: 'install manager'
                  }
                }
              }
            }
          }
        }
        stage('install_cluster'){
          environment {
            CONTAINER_NAME = "cfy"
            IMAGE_NAME = "cfy_manager_image"
            CLUSTER_IMAGE = "cfy_manager_image_preinstalled"
          }
          steps {
            catchError(message: 'Failure on: manager-install install_cluster', buildResult: 'SUCCESS', stageResult: 'FAILURE') {
              container('py27') {
                dir("${env.WORKSPACE}/${env.PROJECT}/jenkins") {
                  withVault([configuration: configuration, vaultSecrets: secrets]) {
                    script {
                      def filename = 'cfy_cluster_config.yaml'
                      def data = readYaml file: filename

                      data.manager_rpm_path = "https://cloudify-release-eu.s3.amazonaws.com/cloudify/${env.S3_BUILD_PATH}/cloudify-manager-install-${env.VERSION}-${env.PRERELEASE}.el7.centos.x86_64.rpm"
                      sh "rm $filename"
                      writeYaml file: filename, data: data
                    }
                    sh script: """#!/bin/bash
                    scp -i ~/.ssh/ec2_ssh_key ${env.WORKSPACE}/${env.PROJECT}/jenkins/cfy_cluster_config.yaml centos@\$(cat capabilities.json | jq '.endpoint.value' | tr -d '"'):/tmp/
                    ssh -i ~/.ssh/ec2_ssh_key -l centos \$(cat capabilities.json | jq '.endpoint.value' | tr -d '"') /bin/bash << 'EOT'
cd ~/cloudify-manager-install && git checkout ${env.BRANCH_NAME}
sudo cp -a /tmp/cfy_cluster_config.yaml /root/
pushd jenkins
  sudo cp -a Dev-PS_cloudify_license.yaml /root/
popd
sudo -i
docker network create --subnet=172.12.0.0/16 cluster_net
ssh-keygen -f ~/pkey.pem -P ""
echo 'Build Docker images'
pushd /root
  docker build --tag sshhost . -f /home/centos/${env.PROJECT}/jenkins/Dockerfiles/host/Dockerfile
  docker build --tag sshclient . -f /home/centos/${env.PROJECT}/jenkins/Dockerfiles/client/Dockerfile
popd
echo 'Create and prepare containers'
docker run -d --name cluster1 --net cluster_net --privileged --ip 172.12.0.3 sshhost
docker run -d --name cluster2 --net cluster_net --privileged --ip 172.12.0.4 sshclient
docker run -d --name cluster3 --net cluster_net --privileged --ip 172.12.0.5 sshclient
docker exec cluster1 systemctl start haveged
docker exec -d cluster1 /usr/sbin/sshd -D
docker exec -d cluster2 /usr/sbin/sshd -D
docker exec -d cluster3 /usr/sbin/sshd -D
echo 'Run Cluster installation'
docker exec --tty cluster1 cfy_cluster_manager install -v
sleep 40
echo 'Check cluster status'
/home/centos/${env.PROJECT}/jenkins/validate_status.sh cluster1 cluster2 cluster3
""", label: 'Prepare and run Cloudify cluster installation'
                  }
                }
              }
            }
          }
        }
    stage('Terminate py27 instnace'){
      steps{
        container('py27'){
          dir("${env.WORKSPACE}/${env.PROJECT}/jenkins"){
            withVault([configuration: configuration, vaultSecrets: secrets]){
              echo 'Uninstall and delete blueprint from manager'
              sh """#!/bin/bash
                source .venv/bin/activate
                cfy uninstall ec2-manager-install-blueprint-${env.BRANCH_NAME}-${env.BUILD_NUMBER} --force --allow-custom-parameters -p ignore_failures=true
              """
            }
          }
        }
      }
    }
  }
}