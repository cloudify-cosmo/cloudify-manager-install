def configuration = [vaultUrl: "${VAULT_URL}",  vaultCredentialId: "vault-app-role", engineVersion: 2]

def secrets = [
  [path: 'secret/jenkins/jenkinstoken', engineVersion: 2, secretValues: [
    [envVar: 'JENKINS_USERNAME', vaultKey: 'username'],
    [envVar: 'JENKINS_TOKEN', vaultKey: 'token']]],
  [path: 'secret/jenkins/cloudifyaws', engineVersion: 2, secretValues: [
    [envVar: 'MANAGER_USERNAME', vaultKey: 'username'],
    [envVar: 'MANAGER_TENANT', vaultKey: 'tenant'],
    [envVar: 'MANAGER_IP', vaultKey: 'ip'],
    [envVar: 'MANAGER_PASSWORD', vaultKey: 'password']]],
]

@Library('pipeline-shared-library') _

pipeline {
  agent {
    kubernetes {
      defaultContainer 'jnlp'
      yamlFile 'jenkins/build-pod.yaml'
    }
  }

  options {
    checkoutToSubdirectory('cloudify-manager-install')
    buildDiscarder(logRotator(numToKeepStr:'10'))
    timeout(time: 60, unit: 'MINUTES')
    timestamps()
  }

  environment {
    PROJECT = "cloudify-manager-install"
    PATH = "/root/.local/bin:$PATH"
    VERSION = getVersion("${env.PROJECT}", "${env.BRANCH_NAME}").trim()
    PRERELEASE = getPreRelease("${env.PROJECT}", "${env.BRANCH_NAME}").trim()
    CLOUDIFY_VERSION = "${env.VERSION}"
    CLOUDIFY_PACKAGE_RELEASE = "${env.PRERELEASE}"
    CONTAINER_NAME = "cfy"
    CLUSTER_IMAGE = "cfy_manager_image_preinstalled"
    S3_BUILD_PATH = "${env.VERSION}/${env.PRERELEASE}-build/${env.PROJECT}/${env.BRANCH_NAME}/${env.BUILD_NUMBER}"
  }
  stages{
    stage ('compatability and flake8') {
      parallel{
        stage('flake8') {
          steps{
            sh script: "mkdir -p ${env.WORKSPACE}/flake8 && cp -rf ${env.WORKSPACE}/${env.PROJECT}/. ${env.WORKSPACE}/flake8", label: "copying repo to seperate workspace"
            container('py36'){
              dir("${env.WORKSPACE}/flake8") {
                echo 'install flake 8'
                sh 'pip install flake8 --user'
                echo 'run flake8'
                sh 'python -m flake8'
              }
            }
          }
        }
        stage('fetch_rpms') {
          steps {
            sh script:"mkdir -p ${env.WORKSPACE}/rpms", label: "creating RPMs folder"
            container('python') {
              dir("${env.WORKSPACE}/rpms") {
                withVault([configuration: configuration, vaultSecrets: secrets]) {
                  echo 'Install requests'
                  sh 'pip install requests'
                  sh script:"""#!/bin/bash
                     source ${env.WORKSPACE}/${env.PROJECT}/packaging/source_branch
                     chmod +x ${env.WORKSPACE}/${env.PROJECT}/jenkins/fetch_rpms
                     ${env.WORKSPACE}/${env.PROJECT}/jenkins/fetch_rpms -u ${env.JENKINS_USERNAME} -t ${env.JENKINS_TOKEN} -d ${env.WORKSPACE}/rpms
                     """, label: 'Fetch RPM artifacts stored on Jenkins'
                }
              }
            }
          }
        }
        stage('build_prometheus_rpms') {
          steps {
            sh script: "mkdir -p ${env.WORKSPACE}/prometheus && cp -rf ${env.WORKSPACE}/${env.PROJECT}/. ${env.WORKSPACE}/prometheus", label: "copying repo to seperate workspace"
            container('rpmbuild') {
              dir("${env.WORKSPACE}/prometheus") {
                echo 'Link items from workspace under rpm sources dir'
                sh """
                  rm ~/rpm -fr
                  ln -s ${env.WORKSPACE}/rpms ./rpms
                  ln -s . ~/rpm
                  """
                echo 'Install rpmdevtools and build dependencies'
                sh '''
                  yum install rpmdevtools -y
                  yum-builddep -y packaging/install_rpm.spec
                  '''
                echo 'build rpms'
                sh """
                  for file in prometheus prometheus_node_exporter prometheus_postgres_exporter prometheus_blackbox_exporter; do
                    echo "build \$file rpm"
                    rpmbuild -bb packaging/\$file.spec
                  done
                  """
                sh script:("mkdir -p ${env.WORKSPACE}/prometheus-rpms && cp -rf /root/rpmbuild/RPMS/x86_64/*.x86_64.rpm ${env.WORKSPACE}/prometheus-rpms"), label:'Copy RPMS to rpms folder'
              }
            }
          }
          post {
            success {
              archiveArtifacts '**/prometheus-rpms/*.rpm'
            }
          }
        }
      }
    }
    stage('build_rpm & deploy ec2 instance') {
      parallel {
        stage('build_rpm') {
          steps {
            container('rpmbuild') {
              sh """
                cd && cd rpmbuild
                git clone --single-branch --branch ${env.BRANCH_NAME} https://github.com/cloudify-cosmo/cloudify-manager-install.git SOURCES && cd SOURCES
                """
              echo 'Setup Github SSH key'
              setupGithubSSHKey()
              echo 'Link items from workspace under rpm sources dir'
              sh """
                cd /root/rpmbuild/SOURCES
                rm ~/rpm -fr
                ln -s ${env.WORKSPACE}/rpms ./rpms
                cp -a ${env.WORKSPACE}/prometheus-rpms/prometheus-*.rpm ./rpms/
                cp -a ${env.WORKSPACE}/prometheus-rpms/*_exporter-*.rpm ./rpms/
                ln -s . ~/rpm
                """
              sh script: """
                pushd rpms
                  /root/rpmbuild/SOURCES/packaging/fetch_requirements --edition premium -b ${BRANCH_NAME} >~/fetch_requirements.log
                  cat ~/fetch_requirements.log
                popd
                """, label: 'fetching requirements'
              echo 'prepare to build rpm'
              sh '''
                yum install rpmdevtools -y
                chmod a+wx /opt
                '''
              echo 'Installing build dependencies'
              sh 'yum-builddep -y /root/rpmbuild/SOURCES/packaging/install_rpm.spec'
              echo 'set premium_edition in the packaged config.yaml'
              sh 'sed -i "s/set_by_installer_builder/premium/" /root/rpmbuild/SOURCES/config.yaml'
              echo 'Build RPM'
              sh """
                cd /root/rpmbuild/SOURCES/
                rpmbuild -D "CLOUDIFY_VERSION ${CLOUDIFY_VERSION}" \
                -D "CLOUDIFY_PACKAGE_RELEASE ${CLOUDIFY_PACKAGE_RELEASE}" \
                -bb packaging/install_rpm.spec
                """
              sh script:("mkdir -p ${env.WORKSPACE}/manager-rpm && cp -rf /root/rpmbuild/RPMS/x86_64/*-manager-*.x86_64.rpm ${env.WORKSPACE}/manager-rpm"), label:'Copy RPMS to rpms folder'
            }
          }
          post {
            success {
              echo 'Upload artifacts to S3'
              uploadToReleaseS3("${env.WORKSPACE}/manager-rpm/","${env.S3_BUILD_PATH}")
            }
          }
        }
        stage('Deploy ec2 instance'){
          steps {
            script {
              buildState = 'FAILURE'
              catchError(message: 'Failure on: manager-install EC2 Creation', buildResult: 'SUCCESS', stageResult: 'FAILURE') {
                container('py36') {
                  echo 'Setup Github SSH key'
                  setupGithubSSHKey()
                  dir("${env.WORKSPACE}/${env.PROJECT}/jenkins") {
                    withVault([configuration: configuration, vaultSecrets: secrets]){
                      sh script:"""#!/bin/bash
                      python -m venv .venv
                      source .venv/bin/activate
                      pip install --upgrade pip
                      pip install cloudify==5.1.2
                      apt-get update
                      apt-get install -y jq
                      cfy profile use ${env.MANAGER_IP} -u ${env.MANAGER_USERNAME} -p ${env.MANAGER_PASSWORD} -t ${env.MANAGER_TENANT} --ssl
                      pushd 'bp'
                        cfy install -b ec2-manager-install-blueprint-${env.BRANCH_NAME}-${env.BUILD_NUMBER} ec2-manager-install-blueprint.yaml
                      popd
                      cfy deployments capabilities ec2-manager-install-blueprint-${env.BRANCH_NAME}-${env.BUILD_NUMBER} --json > capabilities.json
                      jq -r '.key_content.value' capabilities.json > ~/.ssh/ec2_ssh_key && chmod 600 ~/.ssh/ec2_ssh_key
                      sleep 160
                      ssh-keyscan -H \$(jq -r '.endpoint.value' capabilities.json) >> ~/.ssh/known_hosts
                      echo 'ClientAliveInterval 50' >> /etc/ssh/sshd_config
                      """, label:'Configure and install blueprint on manager'
                    }
                  }
                }
                // if We reach here that means everything is ok
                buildState = 'SUCCESS'
              }
            }
          }
        }
      }
    }
    stage ('Configure EC2 Instance') {
      when {
        expression { buildState != 'FAILURE'}
      }
      steps {
        script {
          buildState = 'FAILURE'
          catchError(message: 'Failure on: manager-install install manager', buildResult: 'SUCCESS', stageResult: 'FAILURE') {
            container('py36') {
              dir("${env.WORKSPACE}/${env.PROJECT}/jenkins") {
                withVault([configuration: configuration, vaultSecrets: secrets]) {
                  sh script: """#!/bin/bash
                  ssh -i ~/.ssh/ec2_ssh_key -l centos \$(cat capabilities.json | jq '.endpoint.value' | tr -d '"') /bin/bash << 'EOT'
                  sudo yum -y update
                  sudo service docker start
                  git clone https://github.com/cloudify-cosmo/cloudify-manager-install.git && cd cloudify-manager-install && git checkout ${env.BRANCH_NAME}
                  """, label: 'Configure EC2 Instance'
                }
              }
            }
            // if We reach here that means everything is ok
            buildState = 'SUCCESS'
          }
        }
      }
    }
    stage('install manager & cluster'){
      parallel{
        stage('install_manager') {
          when {
            expression { buildState != 'FAILURE'}
          }
          environment {
            CONTAINER_NAME = "cfy_aio"
            IMAGE_NAME = "cfy_manager_image"
          }
          steps {
            script{
              buildState = 'FAILURE'
              catchError(message: 'Failure on: manager-install install manager', buildResult: 'SUCCESS', stageResult: 'FAILURE') {
                container('py36') {
                  dir("${env.WORKSPACE}/${env.PROJECT}/jenkins") {
                    withVault([configuration: configuration, vaultSecrets: secrets]){
                      sh script: """#!/bin/bash
                      ssh -i ~/.ssh/ec2_ssh_key -l centos \$(cat capabilities.json | jq '.endpoint.value' | tr -d '"') /bin/bash << 'EOT'
cd ~/cloudify-manager-install
echo 'Build manager container'
set -eux
ls -la .
pushd packaging/docker
  docker build --network host --build-arg  rpm_file=https://cloudify-release-eu.s3.amazonaws.com/cloudify/${env.S3_BUILD_PATH}/cloudify-manager-install-${env.VERSION}-${env.PRERELEASE}.el7.centos.x86_64.rpm --tag ${env.IMAGE_NAME} .
popd
echo 'Run manager container'
set -eux
docker run --name ${env.CONTAINER_NAME} -d ${env.IMAGE_NAME}
echo "Waiting for ${env.CONTAINER_NAME} to start"
docker exec ${env.CONTAINER_NAME} cfy_manager wait-for-starter
echo 'Check Manager status'
sleep 40
sudo chmod +x jenkins/validate_status.sh
./jenkins/validate_status.sh ${env.CONTAINER_NAME}
echo 'Run the Sanity check'
sudo docker exec ${env.CONTAINER_NAME} cfy_manager sanity-check
""", label: 'install manager'
                    }
                  }
                }
                // if We reach here that means everything is ok
                buildState = 'SUCCESS'
              }
            }
          }
        }
        stage('install_cluster'){
          when {
            expression { buildState != 'FAILURE'}
          }
          environment {
            CONTAINER_NAME = "cfy"
            IMAGE_NAME = "cfy_manager_image"
            CLUSTER_IMAGE = "cfy_manager_image_preinstalled"
          }
          steps {
           script{
            buildState = 'FAILURE'
            catchError(message: 'Failure on: manager-install install_cluster', buildResult: 'SUCCESS', stageResult: 'FAILURE') {
              container('py36') {
                dir("${env.WORKSPACE}/${env.PROJECT}/jenkins") {
                  withVault([configuration: configuration, vaultSecrets: secrets]) {
                    sh script: """#!/bin/bash
                    ssh -i ~/.ssh/ec2_ssh_key -l centos \$(cat capabilities.json | jq '.endpoint.value' | tr -d '"') /bin/bash << 'EOT'
cd ~/cloudify-manager-install
echo 'Build manager container'
set -eux
ls -la .
pushd packaging/docker
  docker build --network host --build-arg rpm_file=https://cloudify-release-eu.s3.amazonaws.com/cloudify/${env.S3_BUILD_PATH}/cloudify-manager-install-${env.VERSION}-${env.PRERELEASE}.el7.centos.x86_64.rpm --tag ${env.CLUSTER_IMAGE} .
popd
set -eux
pushd jenkins/cluster
  echo "###### Prepare name envvars ######"
  export NODE1_NAME="${env.CONTAINER_NAME}_node1"
  export NODE2_NAME="${env.CONTAINER_NAME}_node2"
  export NODE3_NAME="${env.CONTAINER_NAME}_node3"
  export MANAGER1_IP="172.22.0.3"
  export MANAGER2_IP="172.22.0.4"
  export MANAGER3_IP="172.22.0.5"
  export DB1_IP="172.22.0.3"
  export DB2_IP="172.22.0.4"
  export DB3_IP="172.22.0.5"
  export QUEUE1_IP="172.22.0.3"
  export QUEUE2_IP="172.22.0.4"
  export QUEUE3_IP="172.22.0.5"
  echo "###### Create a docker network ######"
  docker network create --subnet=172.22.0.0/24 net1
  echo "###### generate certs ######"
  chmod +x create_certs.sh
  source ./create_certs.sh
  set -eux
  # Prepare Queue1 on Node1
  sed -e "s/CONTAINER_IP/\${QUEUE1_IP}/g" \
      -e "s/QUEUE2_IP/\${QUEUE2_IP}/" \
      -e "s/QUEUE3_IP/\${QUEUE3_IP}/" \
      queue1_config.yaml > queue_1_config.yaml
  cat queue_1_config.yaml
  docker run -d \
    --name \${NODE1_NAME} \
    --network net1 --ip \${QUEUE1_IP} \
    -v \$(pwd)/queue_1_config.yaml:/etc/cloudify/config.yaml \
    -v \$(pwd)/queue1_key.pem:/etc/cloudify/queue_key.pem \
    -v \$(pwd)/queue1_cert.pem:/etc/cloudify/queue_cert.pem \
    -v \$(pwd)/ca.crt:/etc/cloudify/ca.pem \
    ${env.CLUSTER_IMAGE}
  docker exec \${NODE1_NAME} cfy_manager wait-for-starter -c /etc/cloudify/config.yaml
  docker cp queue_1_config.yaml \${NODE1_NAME}:/etc/cloudify/queue_config.yaml
  # Prepare Queue2 on Node 2
  sed -e "s/CONTAINER_IP/\${QUEUE2_IP}/g" \
      -e "s/QUEUE1_IP/\${QUEUE1_IP}/" \
      -e "s/QUEUE3_IP/\${QUEUE3_IP}/" \
      queue2_config.yaml > queue_2_config.yaml
  cat queue_2_config.yaml
  docker run -d \
    --name \${NODE2_NAME} \
    --network net1 --ip \${QUEUE2_IP} \
    -v \$(pwd)/queue_2_config.yaml:/etc/cloudify/config.yaml \
    -v \$(pwd)/queue2_key.pem:/etc/cloudify/queue_key.pem \
    -v \$(pwd)/queue2_cert.pem:/etc/cloudify/queue_cert.pem \
    -v \$(pwd)/ca.crt:/etc/cloudify/ca.pem \
    ${env.CLUSTER_IMAGE}
  docker exec \${NODE2_NAME} cfy_manager wait-for-starter -c /etc/cloudify/config.yaml
  docker cp queue_2_config.yaml \${NODE2_NAME}:/etc/cloudify/queue_config.yaml
  # Prepare Queue3 on Node 3
  sed -e "s/CONTAINER_IP/\${QUEUE3_IP}/g" \
      -e "s/QUEUE1_IP/\${QUEUE1_IP}/" \
      -e "s/QUEUE2_IP/\${QUEUE2_IP}/" \
      queue3_config.yaml > queue_3_config.yaml
  cat queue_3_config.yaml
  docker run -d \
     --name \${NODE3_NAME} \
     --network net1 --ip \${QUEUE3_IP} \
     -v \$(pwd)/queue_3_config.yaml:/etc/cloudify/config.yaml \
     -v \$(pwd)/queue3_key.pem:/etc/cloudify/queue_key.pem \
     -v \$(pwd)/queue3_cert.pem:/etc/cloudify/queue_cert.pem \
     -v \$(pwd)/ca.crt:/etc/cloudify/ca.pem \
    ${env.CLUSTER_IMAGE}
  docker exec \${NODE3_NAME} cfy_manager wait-for-starter -c /etc/cloudify/config.yaml
  docker cp queue_3_config.yaml \${NODE3_NAME}:/etc/cloudify/queue_config.yaml
  # Prepare DB1 on Node1
  sed -e "s/CONTAINER_IP/\${DB1_IP}/g" \
      -e "s/DB1_IP/\${DB1_IP}/g" \
      -e "s/DB2_IP/\${DB2_IP}/g" \
      -e "s/DB3_IP/\${DB3_IP}/g" \
      db_config.yaml > db1_config.yaml
  cat db1_config.yaml
  sudo docker cp db1_config.yaml \${NODE1_NAME}:/etc/cloudify/db_config.yaml
  sudo docker cp db1_key.pem \${NODE1_NAME}:/etc/cloudify/db_key.pem
  sudo docker cp db1_cert.pem \${NODE1_NAME}:/etc/cloudify/db_cert.pem
  sudo docker exec \${NODE1_NAME} cfy_manager configure -c  /etc/cloudify/db_config.yaml
  # Prepare DB2 on Node2
  sed -e "s/CONTAINER_IP/\${DB2_IP}/g" \
      -e "s/DB1_IP/\${DB1_IP}/g" \
      -e "s/DB2_IP/\${DB2_IP}/g" \
      -e "s/DB3_IP/\${DB3_IP}/g" \
      db_config.yaml > db2_config.yaml
  cat db2_config.yaml
  sudo docker cp db2_config.yaml \${NODE2_NAME}:/etc/cloudify/db_config.yaml
  sudo docker cp db2_key.pem \${NODE2_NAME}:/etc/cloudify/db_key.pem
  sudo docker cp db2_cert.pem \${NODE2_NAME}:/etc/cloudify/db_cert.pem
  sudo docker exec \${NODE2_NAME} cfy_manager configure -c /etc/cloudify/db_config.yaml -v
  # Prepare DB3 on Node3
  sed -e "s/CONTAINER_IP/\${DB3_IP}/g" \
      -e "s/DB1_IP/\${DB1_IP}/g" \
      -e "s/DB2_IP/\${DB2_IP}/g" \
      -e "s/DB3_IP/\${DB3_IP}/g" \
      db_config.yaml > db3_config.yaml
  cat db3_config.yaml
  sudo docker cp db3_config.yaml \${NODE3_NAME}:/etc/cloudify/db_config.yaml
  sudo docker cp db3_key.pem \${NODE3_NAME}:/etc/cloudify/db_key.pem
  sudo docker cp db3_cert.pem \${NODE3_NAME}:/etc/cloudify/db_cert.pem
  sudo docker exec \${NODE3_NAME} cfy_manager configure -c /etc/cloudify/db_config.yaml -v
  # Prepare Manager 1 on Node1
  sed -e "s/CONTAINER_IP/\${MANAGER1_IP}/g" \
      -e "s/QUEUE1_IP/\${QUEUE1_IP}/g" \
      -e "s/QUEUE2_IP/\${QUEUE2_IP}/g" \
      -e "s/QUEUE3_IP/\${QUEUE3_IP}/g" \
      -e "s/DB1_IP/\${DB1_IP}/g" \
      -e "s/DB2_IP/\${DB2_IP}/g" \
      -e "s/DB3_IP/\${DB3_IP}/g" \
      manager1_config.yaml > manager_1_config.yaml
  cat manager_1_config.yaml
    # Generate ca encrypted key
  openssl rsa -aes256 -passout pass:secret_ca_password -in ca.key -out ca.encrypted.key
  sudo docker cp manager_1_config.yaml \${NODE1_NAME}:/etc/cloudify/manager_config.yaml
  sudo docker cp manager_1_key.pem \${NODE1_NAME}:/etc/cloudify/manager_key.pem
  sudo docker cp manager_1_cert.pem \${NODE1_NAME}:/etc/cloudify/manager_cert.pem
  sudo docker cp db_client_1_cert.pem \${NODE1_NAME}:/etc/cloudify/manager_postgres_client_cert.pem
  sudo docker cp db_client_1_key.pem \${NODE1_NAME}:/etc/cloudify/manager_postgres_client_key.pem
  sudo docker cp external_key_1.pem \${NODE1_NAME}:/etc/cloudify/manager_external_key.pem
  sudo docker cp external_cert_1.pem \${NODE1_NAME}:/etc/cloudify/manager_external_cert.pem
  sudo docker cp ca.encrypted.key \${NODE1_NAME}:/etc/cloudify/ca_key.pem
  sudo docker exec \${NODE1_NAME} cfy_manager configure -c /etc/cloudify/manager_config.yaml -v
  # Prepare Manager 2 on Node2
  sed -e "s/CONTAINER_IP/\${MANAGER2_IP}/g" \
      -e "s/QUEUE1_IP/\${QUEUE1_IP}/g" \
      -e "s/QUEUE2_IP/\${QUEUE2_IP}/g" \
      -e "s/QUEUE3_IP/\${QUEUE3_IP}/g" \
      -e "s/DB1_IP/\${DB1_IP}/g" \
      -e "s/DB2_IP/\${DB2_IP}/g" \
      -e "s/DB3_IP/\${DB3_IP}/g" \
      manager2_config.yaml > manager_2_config.yaml
  cat manager_2_config.yaml
  sudo docker cp manager_2_config.yaml \${NODE2_NAME}:/etc/cloudify/manager_config.yaml
  sudo docker cp manager_2_key.pem \${NODE2_NAME}:/etc/cloudify/manager_key.pem
  sudo docker cp manager_2_cert.pem \${NODE2_NAME}:/etc/cloudify/manager_cert.pem
  sudo docker cp db_client_2_cert.pem \${NODE2_NAME}:/etc/cloudify/manager_postgres_client_cert.pem
  sudo docker cp db_client_2_key.pem \${NODE2_NAME}:/etc/cloudify/manager_postgres_client_key.pem
  sudo docker cp external_key_2.pem \${NODE2_NAME}:/etc/cloudify/manager_external_key.pem
  sudo docker cp external_cert_2.pem \${NODE2_NAME}:/etc/cloudify/manager_external_cert.pem
  sudo docker cp prometheus_key_2.pem \${NODE2_NAME}:/etc/cloudify/manager_prometheus_key.pem
  sudo docker cp prometheus_cert_2.pem \${NODE2_NAME}:/etc/cloudify/manager_prometheus_cert.pem
  sudo docker cp ca.encrypted.key \${NODE2_NAME}:/etc/cloudify/ca_key.pem
  sudo docker exec \${NODE2_NAME} cfy_manager configure -c /etc/cloudify/manager_config.yaml -v
  # Prepare Manager 3 on Node3
  sed -e "s/CONTAINER_IP/\${MANAGER3_IP}/g" \
      -e "s/QUEUE1_IP/\${QUEUE1_IP}/g" \
      -e "s/QUEUE2_IP/\${QUEUE2_IP}/g" \
      -e "s/QUEUE3_IP/\${QUEUE3_IP}/g" \
      -e "s/DB1_IP/\${DB1_IP}/g" \
      -e "s/DB2_IP/\${DB2_IP}/g" \
      -e "s/DB3_IP/\${DB3_IP}/g" \
      manager3_config.yaml > manager_3_config.yaml
  cat manager_3_config.yaml
  sudo docker cp manager_3_config.yaml \${NODE3_NAME}:/etc/cloudify/manager_config.yaml
  sudo docker cp manager_3_key.pem \${NODE3_NAME}:/etc/cloudify/manager_key.pem
  sudo docker cp manager_3_cert.pem \${NODE3_NAME}:/etc/cloudify/manager_cert.pem
  sudo docker cp db_client_3_cert.pem \${NODE3_NAME}:/etc/cloudify/manager_postgres_client_cert.pem
  sudo docker cp db_client_3_key.pem \${NODE3_NAME}:/etc/cloudify/manager_postgres_client_key.pem
  sudo docker cp external_key_3.pem \${NODE3_NAME}:/etc/cloudify/manager_external_key.pem
  sudo docker cp external_cert_3.pem \${NODE3_NAME}:/etc/cloudify/manager_external_cert.pem
  sudo docker cp prometheus_key_3.pem \${NODE3_NAME}:/etc/cloudify/manager_prometheus_key.pem
  sudo docker cp prometheus_cert_3.pem \${NODE3_NAME}:/etc/cloudify/manager_prometheus_cert.pem
  sudo docker cp ca.encrypted.key \${NODE3_NAME}:/etc/cloudify/ca_key.pem
  sudo docker exec \${NODE3_NAME} cfy_manager configure -c /etc/cloudify/manager_config.yaml -v
popd
echo 'Check cluster status'
sleep 40
sudo /home/centos/${env.PROJECT}/jenkins/validate_status.sh \${NODE1_NAME} \${NODE2_NAME} \${NODE3_NAME}
""", label: 'Prepare and run Cloudify cluster installation'
                    }
                  }
                }
                // if We reach here that means everything is ok
                buildState = 'SUCCESS'
              }
            }
          }
        }
      }
    }
    stage('Terminate EC2 instance'){
      steps{
        script{
          catchError(message: 'Failure on: Tearing down EC2 instance of manager-install ', buildResult: 'FAILURE', stageResult: 'FAILURE') {
            container('py36') {
              dir("${env.WORKSPACE}/${env.PROJECT}/jenkins"){
                withVault([configuration: configuration, vaultSecrets: secrets]) {
                  echo 'Uninstall and delete blueprint from manager'
                  sh """#!/bin/bash
                    source .venv/bin/activate
                    cfy uninstall ec2-manager-install-blueprint-${env.BRANCH_NAME}-${env.BUILD_NUMBER} --force --allow-custom-parameters -p ignore_failures=true
                  """
                }
              }
            }
          }
        }
      }
    }
  }
  post {
    always {
      findText(textFinders: [
        textFinder(regexp: 'Failure on:*', fileSet: '', alsoCheckConsoleOutput: true, buildResult: 'FAILURE')
        ]
      )
    }
  }
}